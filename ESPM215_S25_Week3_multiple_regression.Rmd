---
title: "ESPM 215 Spring 2025 Week 3 (multiple regression)"
author: "Perry de Valpine"
date: "2025-02-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
```

Today's document does not follow Fieberg in full detail. Instead it has skeletons for simulation exercises.

Here are some comments:

- Fieberg seems to assume some understanding of variance-covariance (also simply called "covariance") matrices. We will start building that up as time allows.
- We won't spend time today on multiple testing or testing combinations of parameters (but these are important topics).
- Fieberg throws some linear algebra down for F tests and Wald tests with little explanation. We'll put a pin in those for later.

# Understanding interactions

## Interactions between two factors

### Two factors each with two levels and an interaction between them

```{r}
fruit <- c("apple", "orange")
idioma <- c("english", "spanish")
fruit_params <- c(apple = 2, orange = 2)
idioma_params <- c(english = 1, spanish = 5)
interaction_params <- matrix(c(0, 0,
                               0, 3), nrow = 2, byrow = TRUE)
rownames(interaction_params) <- fruit 
colnames(interaction_params) <- idioma
interaction_params
n_per_group <- 8
data <- expand.grid(fruit = fruit, idioma = idioma)
data <- data[rep(1:nrow(data), n_per_group),]
data$y <- numeric(nrow(data))
data
```


```{r}
sigma <- .1 # error/residual variation about the line (this comes from the "Residual standard error term - 3rd line from the bottom (use this for the sd term))
set.seed(100)
for(i in 1:nrow(data)) {
  data$y[i] <- fruit_params[data$fruit[i]] + idioma_params[data$idioma[i]] + 
    interaction_params[data$fruit[i], data$idioma[i]] + rnorm(1, 0, sigma)
}
data
```

```{r}
fit <- lm(y ~ fruit * idioma, data = data) 
model.matrix(fit)
summary(fit)
anova(fit) # default analysis of variance null hypothesis is that there are no diff between the fruits (does the same for languages and interactions) 
car::Anova(fit) 

fit_reduced <- lm(y ~ fruit + idioma, data = data) #no interactions
fit_full <- lm(y ~ fruit * idioma, data = data)
anova(fit_reduced, fit_full)
```


 : is symbol for single interaction 
 
looking at factors means it will compare the levels to one reference (e.g. intercept is the diff between apple/orange for english speakers then apple/orange for spanish speakers, then english/spanish for apples then english/spanish for oranges). if no interactions, the lines between the mean differences between levels would look parallel if you were to graph it because that would show that when there is a difference between the means of two groups then the same difference occurs at a different level (e.g. if there is a difference between orange and apples at the english level then well see the same difference between orange and apples at the english level)


- Can you make sense of the model matrix? 
- What is being tested by anova? 
- Let's manually set up some nested models.
- Whats up with R's base `anova` vs `car::Anova`?
- Let's try some different parameter choices.

### Three factors each with three levels

```{r}
fruit <- c("apple", "orange", "banana")
idioma <- c("english", "spanish", "mandarin")
fruit_params <- c(apple = 1, orange = 5, banana = 4)
idioma_params <- c(english = 1, spanish = 3, mandarin = 2)
interaction_params <- matrix(c(0, 0, 0,
                               0, 3, -1,
                               0, 2, 1), nrow = 3, byrow=TRUE)
rownames(interaction_params) <- fruit 
colnames(interaction_params) <- idioma
interaction_params
n_per_group <- 8
data <- expand.grid(fruit = fruit, idioma = idioma)
data <- data[rep(1:nrow(data), n_per_group),]
data$y <- numeric(nrow(data))
```

```{r}
set.seed(101)
sigma <- 2
for(i in 1:nrow(data)) {
  data$y[i] <- fruit_params[data$fruit[i]] + idioma_params[data$idioma[i]] + 
    interaction_params[data$fruit[i], data$idioma[i]] + rnorm(1, 0, sigma)
}
data
```

```{r}
fit <- lm(y ~ fruit * idioma, data = data)
model.matrix(fit)
summary(fit)
anova(fit)
car::Anova(fit)
```

- We have some topics to return to.

## Interactions between continuous and factor variables

### Case with two levels

```{r}
#set the simualation parameters
set.seed(102)
fruit <- c("apple", "orange")
fruit_intercepts <- c(apple = 2, orange = 2) #intercept is the same in each fruit
fruit_slopes <- c(apple = .5, orange = .9) #slopes differ 
n_per_group <- 100
X <- runif(n = n_per_group * length(fruit), 1, 3) #simulating from a uniform between 1 and 3
data <- data.frame(X = X, fruit = rep(fruit, n_per_group))
data$y <- numeric(nrow(data))
```

```{r} 
#simulate the data
sigma <- .1
for(i in 1:nrow(data)) {
  data$y[i] <- fruit_intercepts[data$fruit[i]] + 
    fruit_slopes[data$fruit[i]]*data$X[i] + rnorm(1, 0, sigma)
}
data
```

```{r}
#fit the model 
fit <- lm(y ~ X*fruit, data = data)
model.matrix(fit) # the third column is the difference in slopes 
summary(fit)
anova(fit)
car::Anova(fit)
```

- Let's talk about Type I, II, and III sums of squares.
- Let's talk about treatment contrasts vs. sum-to-zero contrasts
- Let's talk about centering `X`. 
# do you always center right off the bat? 
- Choose some other parameter values.

# Interactions between two continuous variables


```{r}
set.seed(103)
sigma <- 2
n <- 50
X1 <- runif(n, 0, 1)
X2 <- runif(n, 0, 1)
Y <- 10 + 0.3 * X1 + 0.7 * X2 - 3 * (X1*X2) + rnorm(n, 0, sigma) 
# 10 is the intercept and -3 is the ne
# if you change the slope of one of the X's then its going to impact the other one aka the effect of X1 depends on the value of X2
data <- data.frame(X1 = X1, X2 = X2, Y = Y)
```

```{r}
fit <- lm(Y ~ X1*X2, data = data)
summary(fit)
```

- Let's talk about centering `X1` and `X2`.

# you should always center (it cant hurt)

# Three flavors of ANOVA:
- type 1 - adds terms in one at a time/sequentially 
e.g. looks at fruit and ignores the other variable (idioma), then adds idioma (has idioma and fruit but not interaction), then adds interaction (has all three terms)
- type 2 - tests all terms except the interaction at first
e.g. includes fruit and idioma but not interaction and then includes interaction (has fruit, idioma, and interaction)
- type 3 - inclusive of all terms, but this is hard to interpret unless you use the sum to zero parameterization (aka contrasts)
e.g includes fruit, idioma, and interaction
- sums of squares




# Collinearity and confounding

Collinearity and confounding both involve *correlated* explanatory variables.

We'll simulate with the multivariate normal.

```{r}
set.seed(104)
n <- 200
library(mvtnorm)
vcov <- matrix(c(1.5, .75,
                 .75, 1.5), nrow=2, byrow=TRUE)
mu <- c(1, 3)
X <- rmvnorm(n, mean = mu, sigma = vcov)
X1 <- X[,1]
X2 <- X[,2]
plot(X1, X2)
cor(X)
```

```{r}
set.seed(105)
sigma <- 2
Y <- 10 + 0.3 * X1 + 0.7 * X2 + rnorm(n, 0, sigma)
data <- data.frame(X1 = X1, X2 = X2, Y = Y)
```

- Let's look at model fits with both or only one of the `X`s.
- Look at parameter estimates, standard errors, and P-values.

# Number of estimated parameters

- Let's see what happens when we estimate more and more parameters.

# Bias-variance tradeoffs

- Is it ever better to ignore some variables?