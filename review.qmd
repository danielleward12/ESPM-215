---
title: "Review"
format: html
editor: visual
---

# Major concepts from first class:

## simulating data

we simulate so we can compare the linear regression values from our data with random simulated data to determine the likelihood of our data

## sampling distribution

sampling distribution is the distribution of parameter estimates across multiple samples of data

The correct parameters will be in the middle of the histogram and as well as the points when the estimate of intercept is plotted against estimate of slope.

aka is the distribution of sample statistics computed for different samples of the same size, from the same population (e.g. the mean of many different simulated samples)

## sampling error

standard error is the standard deviation of the sampling distribution to find the true standard error you can increase the number of simulations

## p-value

is how probable that null hypothesis (slope is 0) is true or at least as extreme p values are helpful if we are skeptical about the conclusion

## intercept

the average value of Y when all predictors are set equal to 0

## slope

predicted change in Y per unit increase in X


## notes

when we look at a model summary we'll see Std. Error, t value, and Pr(>|t|) columns under the coefficients heading
the first row (Intercept) is related to null hypothesis testing
the second row provides a test statistic and p-value for testing whether the slope parameter is 0


# Questions:

In class perry said "simulating data allows us to ask how methods perform across a distribution of data" - what does this mean? arent we looking at how our data perform across a distribution?

how do you know when to use linear regressions? - (also when looking at just two variables?) residuals normally distributed


# Major concepts class 2:


interactions = the differences between one variable depends on another variable
(or the effect of one variable on another depends on a third variables)
- can be two categorical
- two continuous
- one of each 

centering continuous variables = subtracting the mean from each observation (effectively putting 0 right in the middle of the x axis so then youll be able to see y right at the mean of x and the difference you get is the difference between y right at the mean of x)
standardizing = subtracting the mean and dividing by the standard deviation



# Diagnosing residuals

1. residual versus fitted value plot for linearity
2. Q-Q plot (quantiles of the standardized residuals versus quantiles of a standard normal distribution) for noramlity 
3. leverage (how far away each observation is in “predictor space” from the other observations) Observations with high leverage are often, but not always, highly influential points and are indicated with their row numbers) 
4. residual versus standardized residuals also for linearity but especially for small data sets




